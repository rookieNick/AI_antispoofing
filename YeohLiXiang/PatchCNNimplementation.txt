3.3.1.2 Implementation of Patch-based Convolution Neural Network

This document details the implementation of a Patch-based Convolutional Neural Network (CNN) for face anti-spoofing, drawing insights from the provided `train_patch_cnn.py` and `model_patch_cnn.py` scripts. The implementation focuses on a patch-based approach, which is particularly effective for capturing local texture details crucial for distinguishing between live and spoofed faces.

3.3.1.2.1 Configuration and Hyperparameters

The training process is governed by a set of carefully selected parameters to ensure stable convergence, robust performance, and efficient resource utilization. The key parameters are summarized below:

*   **BATCH_SIZE**: 128. An increased batch size is used for faster training, balancing computational efficiency with gradient stability.
*   **IMAGE_SIZE**: (112, 112). Images are resized to 112x112 pixels. This resolution is chosen for computational efficiency while retaining sufficient detail for patch-based analysis.
*   **EPOCHS**: 100. The model is trained for an extended period of 100 epochs to allow for thorough convergence and the learning of complex patterns. This is paired with an early stopping mechanism to prevent overfitting, ensuring the model generalizes well to unseen data.
*   **LEARNING_RATE**: 0.0001. A lower learning rate is employed to ensure better convergence and prevent overshooting optimal weights, suitable for the training dynamics of the patch-based CNN.
*   **WEIGHT_DECAY**: 0.01. L2 regularization is applied through weight decay to prevent overfitting, encouraging the model to learn more generalized patterns.
*   **PATIENCE**: 5. The early stopping mechanism is configured with a patience of 5 epochs. This allows the model to continue training for a specified number of epochs even if validation performance does not improve, preventing premature termination and enabling the model to escape temporary plateaus in its learning curve.
*   **LABEL_SMOOTHING**: 0.0. Label smoothing is disabled for initial runs, simplifying the loss calculation.
*   **SAMPLE_LIMIT**: 5000. Limit training dataset size to the first 5,000 images to speed up experimentation.

3.3.1.2.2 Data Augmentation

To prepare and augment the data, a set of standard transformations is applied. The current implementation focuses on basic resizing, conversion to tensor, and normalization.

*   **Resize**: Images are resized to the configured `IMAGE_SIZE` (112x112 pixels). This ensures all inputs have a consistent spatial dimension for the model.
*   **ToTensor**: Converts PIL Images to PyTorch tensors, scaling pixel values from [0, 255] to [0, 1] and changing the image format from HWC (Height, Width, Channels) to CHW (Channels, Height, Width), which is the standard input format for PyTorch models.
*   **Normalization**: Images are normalized using ImageNet statistics (Mean: [0.485, 0.456, 0.406], Std: [0.229, 0.224, 0.225]). This standardizes the input distribution, which is crucial for stable training and compatibility with pre-trained models.

3.3.1.2.3 Dataset Splitting and Handling

Effective dataset management is crucial for training robust anti-spoofing models. The following techniques are employed for dataset splitting and handling:

*   **Dataset Split**: The dataset is split into an 80% training set and a 20% validation set. A fixed random seed of 42 is used to ensure reproducible splits across multiple runs, allowing for consistent monitoring of overfitting and reliable evaluation of model performance.
*   **Class Imbalance Handling**: The dataset exhibits class imbalance, with a higher number of spoof images compared to live images. To address this, inverse-frequency weights are calculated for each class. This assigns higher weights to samples from the minority class (live images), preventing the model from being biased towards the majority class (spoof images) and improving its ability to learn from underrepresented examples.
*   **Weighted Random Sampling**: While inverse-frequency weights are calculated, the current implementation uses `shuffle=True` in the `DataLoader` instead of a `WeightedRandomSampler`. This means batches are randomly shuffled, but not explicitly balanced by class weight during sampling.

3.3.1.2.4 Patch-based CNN Architecture

The Patch-based CNN architecture is designed to effectively extract hierarchical features from facial images, with a specific focus on local patch-based analysis for anti-spoofing. The architecture, as defined in `model_patch_cnn.py`, consists of shared convolutional blocks for patch processing, followed by feature aggregation and a final classifier. The `PatchBasedCNN` class implements this architecture.

**1. Patch Extraction**:
The input image is first divided into a grid of overlapping or non-overlapping patches. The `extract_patches` method in `PatchBasedCNN` handles this, taking an input tensor of shape `(batch_size, channels, height, width)` and returning a tensor of shape `(batch_size, num_patches, channels, patch_size, patch_size)`. For an `IMAGE_SIZE` of (112, 112) and `PATCH_SIZE` of 32, with `NUM_PATCHES` of 9 (a 3x3 grid), the patches are extracted systematically.

**2. Shared CNN Backbone (Patch-level Feature Extraction)**:
Each extracted patch is then fed through a shared convolutional neural network backbone. This backbone is designed to capture local spatial features such as texture and color variations within each patch. The `patch_cnn` sequential module in `PatchBasedCNN` comprises three convolutional blocks:

*   **Block 1**:
    *   [`Conv2d(3, 32, kernel_size=3, padding=1)`](YeohLiXiang/model_patch_cnn.py:30): Takes 3 input channels (RGB) and produces 32 output filters. A 3x3 kernel with 1-pixel padding helps capture local features while preserving spatial dimensions.
    *   [`BatchNorm2d(32)`](YeohLiXiang/model_patch_cnn.py:31): Normalizes feature maps to stabilize and accelerate training, crucial for handling varied lighting and spoofing conditions.
    *   [`ReLU(inplace=True)`](YeohLiXiang/model_patch_cnn.py:32): A non-linear activation function.
    *   [`Conv2d(32, 32, kernel_size=3, padding=1)`](YeohLiXiang/model_patch_cnn.py:33): Another convolutional layer to deepen the feature extraction.
    *   [`BatchNorm2d(32)`](YeohLiXiang/model_patch_cnn.py:34)
    *   [`ReLU(inplace=True)`](YeohLiXiang/model_patch_cnn.py:35)
    *   [`MaxPool2d(2, 2)`](YeohLiXiang/model_patch_cnn.py:36): Reduces spatial dimensions by half, allowing the network to focus on high-level features.
    *   [`Dropout2d(0.25)`](YeohLiXiang/model_patch_cnn.py:37): Applies dropout to feature maps to prevent overfitting.

*   **Block 2**:
    *   [`Conv2d(32, 64, kernel_size=3, padding=1)`](YeohLiXiang/model_patch_cnn.py:40): Increases the number of filters to 64, enabling the capture of more complex patterns.
    *   [`BatchNorm2d(64)`](YeohLiXiang/model_patch_cnn.py:41)
    *   [`ReLU(inplace=True)`](YeohLiXiang/model_patch_cnn.py:42)
    *   [`Conv2d(64, 64, kernel_size=3, padding=1)`](YeohLiXiang/model_patch_cnn.py:43)
    *   [`BatchNorm2d(64)`](YeohLiXiang/model_patch_cnn.py:44)
    *   [`ReLU(inplace=True)`](YeohLiXiang/model_patch_cnn.py:45)
    *   [`MaxPool2d(2, 2)`](YeohLiXiang/model_patch_cnn.py:46)
    *   [`Dropout2d(0.25)`](YeohLiXiang/model_patch_cnn.py:47)

*   **Block 3**:
    *   [`Conv2d(64, 128, kernel_size=3, padding=1)`](YeohLiXiang/model_patch_cnn.py:50): Further increases filters to 128 to capture even more discriminative patterns like micro-textures and depth cues.
    *   [`BatchNorm2d(128)`](YeohLiXiang/model_patch_cnn.py:51)
    *   [`ReLU(inplace=True)`](YeohLiXiang/model_patch_cnn.py:52)
    *   [`MaxPool2d(2, 2)`](YeohLiXiang/model_patch_cnn.py:53)
    *   [`Dropout2d(0.25)`](YeohLiXiang/model_patch_cnn.py:54)

After these convolutional blocks, the feature maps are flattened, and then passed through a patch-level fully connected layer (`patch_fc`) to extract a 128-dimensional feature vector for each patch.

**3. Feature Aggregation**:
The features from all individual patches are then concatenated to form a single, comprehensive feature vector. This combined vector is fed into an aggregation module (`aggregation`) consisting of two linear layers with ReLU activations and dropout. This module learns to combine the local patch information into a global representation.

**4. Classifier**:
Finally, a linear output layer (`classifier`) maps the aggregated features to the number of classes (e.g., 2 for real vs. spoof), producing the final logits for classification.

**Weight Initialization**:
The model's weights are initialized using `kaiming_normal_` for convolutional layers and `normal_` for linear layers, with biases initialized to zero or one for BatchNorm layers. This helps in stable training from the outset.

**PatchDepthCNN (Enhanced Version)**:
The `model_patch_cnn.py` also includes an enhanced `PatchDepthCNN` which incorporates an attention mechanism to weight the importance of different patches. This model uses higher filter counts (64, 128, 256) in its convolutional blocks and an explicit attention module to compute weights for each patch's features before aggregation. This allows the model to dynamically focus on the most informative regions for anti-spoofing.

3.3.1.2.5 Loss Functions and Optimization

The training process employs a sophisticated loss function and optimization strategy to maximize accuracy and robustness.

*   **Loss Function**: The model utilizes Cross Entropy Loss.
    *   **Cross Entropy Loss with Class Weights**: This serves as the primary classification loss. Class weights, calculated based on inverse-frequency, are applied to address class imbalance, assigning higher importance to underrepresented classes. Label smoothing is currently disabled (set to 0.0).

*   **Optimizer**: The Adam optimizer is used.
    *   **Learning Rate**: Initialized at 0.0001.
    *   **Weight Decay**: Set to 0.01 to minimize overfitting.
    *   **Momentum Parameters**: Standard `betas=(0.9, 0.999)` and `eps=1e-8` are used for stable convergence.

*   **Learning Rate Scheduler**: No learning rate scheduler is currently employed. The learning rate remains constant throughout training.

3.3.1.2.6 Training Loop Implementation

The training loop is meticulously designed for stability, efficiency, and robustness, incorporating several advanced techniques.

*   **Phases**: Each epoch consists of two primary phases: training and validation.
    *   **Training Phase**: The model is set to `model.train()` mode, enabling features like dropout and batch normalization updates, and allowing for gradient computation.
    *   **Validation Phase**: The model is set to `model.eval()` mode, disabling dropout and ensuring batch normalization uses global statistics, and gradients are not computed (`torch.no_grad()`).

*   **Advanced Data Augmentation**: The current implementation does not include advanced data augmentation techniques such as Mixup or CutMix.

*   **Mixed Precision Training**: The training loop incorporates mixed precision training, utilizing 16-bit floating-point precision for most operations. This significantly accelerates training on modern GPUs and reduces memory usage without compromising accuracy. `torch.cuda.amp.GradScaler` is used to manage automatic loss scaling and gradient unscaling, maintaining numerical stability.

*   **Gradient Clipping**: Gradient clipping with a norm of 1.0 is applied to prevent exploding gradients, ensuring controlled updates and stable convergence.

*   **Metrics Logging**: During training, various metrics such as training loss, training accuracy, validation loss, validation accuracy, precision, and recall are calculated and logged using a `MetricsLogger`. This provides a comprehensive overview of the model's performance and helps in monitoring progress and identifying potential issues.

3.3.1.2.7 Model Saving and Checkpointing

A robust model saving and checkpointing strategy is implemented to ensure efficient and reliable training.

*   **Best Model Tracking**: The validation accuracy is continuously monitored throughout training. Whenever the current validation accuracy surpasses the previously recorded best, the model's parameters (state dictionary) are saved using `torch.save(model.state_dict(), model_path)`. Saving only the state dictionary ensures memory efficiency and portability.
*   **Early Stopping**: An early stopping mechanism is integrated, monitoring the validation accuracy for improvement over a predefined number of epochs (patience of 5). If no improvement is observed within this window, training is automatically terminated. This prevents unnecessary computation, saves resources, and significantly reduces the risk of overfitting, which is critical for anti-spoofing systems where overfitting to training data can lead to poor performance against novel spoofing techniques. Early stopping ensures that the final deployed model is both efficient and robust, maintaining strong generalization across diverse spoofing attempts.