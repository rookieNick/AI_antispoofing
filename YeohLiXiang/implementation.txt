===============================================================================
COMPREHENSIVE CNN IMPLEMENTATION GUIDE FOR FACE ANTI-SPOOFING
===============================================================================

This document provides an in-depth, step-by-step explanation of the CNN implementation 
in train_fine_tuned.py for face anti-spoofing using the CASIA-FASD dataset.

===============================================================================
TABLE OF CONTENTS
===============================================================================
1. OVERVIEW AND ARCHITECTURE
2. CONFIGURATION AND HYPERPARAMETERS
3. DATA PREPROCESSING PIPELINE
4. ADVANCED DATA AUGMENTATION TECHNIQUES
5. DATASET LOADING AND MANAGEMENT
6. MODEL ARCHITECTURE AND COMPONENTS
7. LOSS FUNCTIONS AND OPTIMIZATION
8. TRAINING LOOP IMPLEMENTATION
9. VALIDATION AND METRICS
10. MODEL SAVING AND CHECKPOINTING
11. PLOTTING AND VISUALIZATION
12. ADVANCED TECHNIQUES EXPLAINED

===============================================================================
1. OVERVIEW AND ARCHITECTURE
===============================================================================

1.1 PROBLEM DEFINITION
The face anti-spoofing task is implemented as a binary classification problem:
- Class 0: Live (real human faces)
- Class 1: Spoof (fake faces from photos, videos, masks, etc.)

1.2 OVERALL ARCHITECTURE FLOW
Input Image → Data Preprocessing → Data Augmentation → CNN Model → 
Loss Calculation → Backpropagation → Optimization → Validation → 
Metrics Calculation → Model Saving → Results Visualization

1.3 KEY DESIGN DECISIONS
- Binary classification approach rather than object detection
- High-resolution input (128x128) to capture fine facial details
- Multiple advanced data augmentation techniques
- Combined loss functions (Cross Entropy + Focal Loss)
- Mixed precision training for efficiency
- Cosine annealing with warm restarts for optimal learning rate scheduling

===============================================================================
2. CONFIGURATION AND HYPERPARAMETERS
===============================================================================

2.1 TRAINING PARAMETERS
The configuration section defines all hyperparameters optimized for maximum accuracy:

BATCH_SIZE = 64
- Smaller batch size chosen for better gradient estimates
- Provides more frequent parameter updates
- Better for limited data scenarios
- Improves generalization capability

IMAGE_SIZE = (128, 128)
- Higher resolution than typical (224x224 reduced to 128x128 for efficiency)
- Captures finer facial details crucial for anti-spoofing
- Balances detail preservation with computational efficiency
- Sufficient to detect texture differences between live and spoof faces

EPOCHS = 100
- Extended training for thorough convergence
- Allows model to learn complex patterns
- Combined with early stopping to prevent overfitting

LEARNING_RATE = 0.0005
- Conservative learning rate for stable convergence
- Prevents overshooting optimal weights
- Suitable for fine-tuning scenarios
- Works well with cosine annealing scheduler

WEIGHT_DECAY = 0.005
- L2 regularization to prevent overfitting
- Reduced value allows model flexibility
- Balances between underfitting and overfitting

2.2 ADVANCED TRAINING TECHNIQUES

PATIENCE = 15
- Early stopping patience increased for thorough training
- Prevents premature training termination
- Allows model to escape temporary plateaus

LABEL_SMOOTHING = 0.15
- Prevents overconfident predictions
- Improves model calibration
- Enhances generalization to unseen data
- Particularly effective for binary classification

GRADIENT_CLIP_NORM = 0.5
- Prevents exploding gradients in deep networks
- Ensures stable training with advanced augmentations
- Tighter clipping for more controlled updates

2.3 MIXUP/CUTMIX PARAMETERS

MIXUP_ALPHA = 0.4
- Controls mixing intensity between samples
- Higher values create more aggressive blending
- Improves model robustness and generalization

CUTMIX_ALPHA = 1.0
- Controls size of cut regions between samples
- Encourages model to focus on multiple facial regions
- Reduces reliance on specific image areas

MIXUP_PROB = 0.5
- 50% probability of applying mixing augmentation
- Balances augmented vs. original samples
- Maintains training data diversity

2.4 DATA LOADING OPTIMIZATION

NUM_WORKERS = 8
- Parallel data loading processes
- Reduces GPU idle time during training
- Optimizes data pipeline efficiency

PIN_MEMORY = True
- Keeps data in pinned memory for faster GPU transfer
- Reduces CPU-GPU communication overhead
- Only beneficial when using GPU training

PERSISTENT_WORKERS = True
- Reuses data loading processes between epochs
- Reduces worker creation/destruction overhead
- Improves overall training efficiency

===============================================================================
3. DATA PREPROCESSING PIPELINE
===============================================================================

3.1 DIRECTORY STRUCTURE HANDLING
The implementation expects a specific directory structure:
dataset/casia-fasd/train/
├── live/          # Real face images
└── spoof/         # Fake face images

3.2 PATH CONSTRUCTION
```python
script_dir = os.path.dirname(os.path.abspath(__file__))
dataset_dir = os.path.join(script_dir, "..", "dataset", "casia-fasd")
train_dir = os.path.join(dataset_dir, "train")
```

Process:
1. Get current script directory
2. Navigate to parent directory
3. Construct path to dataset
4. Verify dataset existence

3.3 DATASET VALIDATION
```python
if not os.path.exists(train_dir):
    print(f"Error: Training directory not found at {train_dir}")
    return
```

The system validates dataset existence before proceeding with training.

===============================================================================
4. ADVANCED DATA AUGMENTATION TECHNIQUES
===============================================================================

4.1 GAUSSIAN BLUR AUGMENTATION

Custom GaussianBlur Class Implementation:
```python
class GaussianBlur:
    def __init__(self, kernel_size=3, sigma_range=(0.1, 2.0)):
        self.kernel_size = kernel_size
        self.sigma_range = sigma_range
    
    def __call__(self, img):
        sigma = random.uniform(*self.sigma_range)
        return transforms.functional.gaussian_blur(img, self.kernel_size, sigma)
```

Purpose:
- Simulates camera focus variations
- Handles motion blur effects
- Improves robustness to image quality variations
- Particularly relevant for video-based spoofing attacks

Technical Details:
- Kernel size of 3x3 for subtle blur effects
- Sigma range (0.1, 2.0) provides varying blur intensities
- Random sigma selection for diverse augmentation

4.2 TRAINING TRANSFORM PIPELINE

The training transform applies aggressive augmentation:

```python
train_transform = transforms.Compose([
    transforms.Resize((int(IMAGE_SIZE[0] * 1.1), int(IMAGE_SIZE[1] * 1.1))),
    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0), ratio=(0.8, 1.2)),
    transforms.RandomHorizontalFlip(p=HORIZONTAL_FLIP_PROB),
    transforms.RandomRotation(ROTATION_DEGREES, interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.ColorJitter(...),
    transforms.RandomApply([GaussianBlur()], p=GAUSSIAN_BLUR_PROB),
    transforms.RandomGrayscale(p=GRAYSCALE_PROB),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    transforms.RandomErasing(p=RANDOM_ERASING_PROB, scale=(0.02, 0.2), ratio=(0.3, 3.3)),
])
```

Step-by-step breakdown:

1. RESIZE TO LARGER SIZE
   - Resizes to 110% of target size (140x140 for 128x128 target)
   - Provides room for random cropping
   - Maintains aspect ratio

2. RANDOM RESIZED CROP
   - Crops to target size (128x128)
   - Scale range (0.8, 1.0) for zoom variation
   - Ratio range (0.8, 1.2) for aspect ratio variation
   - Simulates different face sizes and positions

3. RANDOM HORIZONTAL FLIP
   - 50% probability of horizontal mirroring
   - Increases data diversity
   - Face orientation invariance
   - Doubles effective dataset size

4. RANDOM ROTATION
   - ±15 degrees rotation range
   - Bilinear interpolation for smooth rotation
   - Simulates head pose variations
   - Handles slight camera angle differences

5. COLOR JITTER
   - Brightness variation: ±30%
   - Contrast variation: ±30%
   - Saturation variation: ±30%
   - Hue variation: ±15%
   - Simulates different lighting conditions and cameras

6. GAUSSIAN BLUR
   - 10% probability of applying blur
   - Simulates focus and motion blur
   - Improves robustness to image quality

7. RANDOM GRAYSCALE
   - 10% probability of grayscale conversion
   - Reduces color dependency
   - Focuses model on texture and structure

8. TENSOR CONVERSION
   - Converts PIL Image to PyTorch tensor
   - Scales pixel values from [0, 255] to [0, 1]
   - Changes format from HWC to CHW

9. NORMALIZATION
   - Uses ImageNet statistics for transfer learning compatibility
   - Mean: [0.485, 0.456, 0.406] for RGB channels
   - Std: [0.229, 0.224, 0.225] for RGB channels
   - Standardizes input distribution

10. RANDOM ERASING
    - 20% probability of erasing rectangular regions
    - Scale range (0.02, 0.2) - small to medium patches
    - Aspect ratio range (0.3, 3.3)
    - Simulates occlusion and improves robustness

4.3 VALIDATION TRANSFORM PIPELINE

Validation uses minimal preprocessing:
```python
val_transform = transforms.Compose([
    transforms.Resize(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
```

Key differences from training:
- No data augmentation
- Direct resize to target size
- Consistent preprocessing for reliable evaluation
- Same normalization for fair comparison

===============================================================================
5. DATASET LOADING AND MANAGEMENT
===============================================================================

5.1 PYTORCH IMAGEFOLDER USAGE

```python
full_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
```

ImageFolder automatically:
- Creates class labels from subdirectory names
- Maps 'live' → 0, 'spoof' → 1
- Loads all supported image formats
- Applies specified transforms

5.2 DATASET SPLITTING STRATEGY

```python
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(
    full_dataset, [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)
```

Implementation details:
- 80/20 train/validation split
- Fixed random seed (42) for reproducible splits
- Ensures same split across multiple runs
- Validation set for monitoring overfitting

5.3 TRANSFORM APPLICATION FOR VALIDATION

```python
val_dataset.dataset.transform = val_transform
```

Critical step:
- Applies validation transforms to validation subset
- Ensures no data augmentation during evaluation
- Maintains data integrity for reliable metrics

5.4 CLASS IMBALANCE HANDLING

```python
train_targets = [full_dataset[i][1] for i in train_dataset.indices]
class_counts = Counter(train_targets)
total_samples = len(train_targets)
class_weights = {cls: total_samples / count for cls, count in class_counts.items()}
sample_weights = [class_weights[target] for target in train_targets]
```

Process:
1. Extract labels for training samples
2. Count samples per class
3. Calculate inverse frequency weights
4. Assign weight to each sample
5. Use weights for balanced sampling

5.5 WEIGHTED RANDOM SAMPLING

```python
sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)
```

Benefits:
- Ensures balanced batches
- Prevents model bias toward majority class
- Improves learning for minority class
- Allows sampling with replacement

5.6 DATALOADER CONFIGURATION

```python
train_loader = DataLoader(
    train_dataset, batch_size=BATCH_SIZE, sampler=sampler,
    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,
    persistent_workers=PERSISTENT_WORKERS
)
```

Optimization features:
- Custom sampler for balanced training
- Multiple workers for parallel loading
- Pinned memory for faster GPU transfer
- Persistent workers to reduce overhead

===============================================================================
6. MODEL ARCHITECTURE AND COMPONENTS
===============================================================================

6.1 MODEL INITIALIZATION

```python
num_classes = len(full_dataset.classes)  # 2 for live/spoof
model = OptimizedCNN(num_classes=num_classes).to(device)
```

The model is imported from a separate module:
- OptimizedCNN: Custom CNN architecture
- Designed specifically for anti-spoofing
- Moved to appropriate device (GPU/CPU)

6.2 CNN ARCHITECTURE CHARACTERISTICS

While the specific architecture is in model.py, typical components include:
- Convolutional layers for feature extraction
- Batch normalization for training stability
- ReLU activations for non-linearity
- Pooling layers for dimensionality reduction
- Dropout layers for regularization
- Fully connected layers for classification

6.3 MODEL DEVICE PLACEMENT

```python
model = OptimizedCNN(num_classes=num_classes).to(device)
```

Process:
- Creates model instance
- Moves all parameters and buffers to specified device
- Enables GPU acceleration if available
- Ensures consistent device placement

===============================================================================
7. LOSS FUNCTIONS AND OPTIMIZATION
===============================================================================

7.1 DUAL LOSS FUNCTION APPROACH

The implementation uses two complementary loss functions:

```python
ce_criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)
focal_criterion = FocalLoss(alpha=1, gamma=2)
```

7.2 CROSS ENTROPY LOSS WITH LABEL SMOOTHING

CrossEntropyLoss with label_smoothing=0.15:
- Standard classification loss
- Label smoothing prevents overconfident predictions
- Improves model calibration
- Formula: CE_smooth = (1-ε) * CE + ε * uniform_distribution
- Where ε = 0.15 (label smoothing factor)

Benefits:
- Reduces overfitting
- Improves generalization
- Better uncertainty estimation
- More robust to label noise

7.3 FOCAL LOSS IMPLEMENTATION

Custom FocalLoss class:
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()
```

Mathematical formulation:
FL(p_t) = -α_t * (1-p_t)^γ * log(p_t)

Where:
- p_t: Model's estimated probability for true class
- α_t: Class-dependent weighting factor (α=1)
- γ: Focusing parameter (γ=2)

Purpose:
- Addresses class imbalance
- Down-weights easy examples
- Focuses training on hard examples
- Particularly effective for binary classification

7.4 COMBINED LOSS STRATEGY

```python
if use_mixup or use_cutmix:
    ce_loss = mixup_criterion(ce_criterion, outputs, targets_a, targets_b, lam)
    focal_loss = mixup_criterion(focal_criterion, outputs, targets_a, targets_b, lam)
    loss = 0.7 * ce_loss + 0.3 * focal_loss
else:
    ce_loss = ce_criterion(outputs, targets)
    focal_loss = focal_criterion(outputs, targets)
    loss = 0.7 * ce_loss + 0.3 * focal_loss
```

Combination strategy:
- 70% Cross Entropy Loss (primary classification signal)
- 30% Focal Loss (hard example focus)
- Weighted combination balances both objectives
- Different handling for mixed vs. original samples

7.5 OPTIMIZER CONFIGURATION

```python
optimizer = optim.AdamW(
    model.parameters(),
    lr=LEARNING_RATE,
    betas=(ADAM_BETA1, ADAM_BETA2),
    eps=ADAM_EPS,
    weight_decay=WEIGHT_DECAY
)
```

AdamW advantages:
- Decoupled weight decay from gradient-based update
- Better regularization than standard Adam
- More stable training
- Improved generalization

Parameters:
- lr=0.0005: Conservative learning rate
- betas=(0.9, 0.999): Standard momentum parameters
- eps=1e-8: Numerical stability
- weight_decay=0.005: L2 regularization

7.6 LEARNING RATE SCHEDULING

```python
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=T_0, T_mult=T_MULT, eta_min=ETA_MIN
)
```

Cosine Annealing with Warm Restarts:
- T_0=10: Initial restart period (epochs)
- T_mult=2: Restart period multiplier
- eta_min=1e-7: Minimum learning rate

Benefits:
- Periodic learning rate restarts
- Helps escape local minima
- Improves final performance
- Automatic learning rate decay

Mathematical formulation:
η_t = η_min + (η_max - η_min) * (1 + cos(π * T_cur / T_i)) / 2

Where:
- η_t: Learning rate at step t
- T_cur: Current epoch within restart period
- T_i: Current restart period length

===============================================================================
8. TRAINING LOOP IMPLEMENTATION
===============================================================================

8.1 TRAINING LOOP STRUCTURE

```python
for epoch in range(EPOCHS):
    # Training phase
    model.train()
    # ... training code ...
    
    # Validation phase  
    model.eval()
    # ... validation code ...
    
    # Metrics and saving
    # ... metrics calculation and model saving ...
```

8.2 TRAINING MODE CONFIGURATION

```python
model.train()
```

Effects:
- Enables dropout layers
- Enables batch normalization updates
- Sets model to training behavior
- Allows gradient computation

8.3 MIXUP/CUTMIX AUGMENTATION

Advanced augmentation applied during training:

```python
if random.random() < MIXUP_PROB:
    if random.random() < 0.5:
        # Apply mixup
        data, targets_a, targets_b, lam = mixup_data(data, targets)
        use_mixup = True
        use_cutmix = False
    else:
        # Apply cutmix
        data, targets_a, targets_b, lam = cutmix_data(data, targets)
        use_mixup = False
        use_cutmix = True
else:
    use_mixup = use_cutmix = False
```

Process:
1. 50% probability of applying mixing
2. If mixing: 50% chance each of Mixup vs CutMix
3. Generate mixed samples and dual targets
4. Track which augmentation was applied

8.4 MIXUP IMPLEMENTATION

```python
def mixup_data(x, y, alpha=MIXUP_ALPHA):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam
```

Process:
1. Sample mixing coefficient λ from Beta(α, α)
2. Create random permutation for sample pairing
3. Linear interpolation: x_mixed = λ * x₁ + (1-λ) * x₂
4. Return mixed samples and original labels

Mathematical foundation:
- Creates convex combinations of training examples
- Encourages linear behavior between training examples
- Improves generalization and reduces overfitting

8.5 CUTMIX IMPLEMENTATION

```python
def cutmix_data(x, y, alpha=CUTMIX_ALPHA):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    y_a, y_b = y, y[index]
    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)
    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
    
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))
    return x, y_a, y_b, lam
```

Process:
1. Sample area ratio from Beta distribution
2. Calculate bounding box coordinates
3. Cut and paste rectangular regions
4. Adjust λ to match actual area ratio

Benefits:
- Combines local features from different images
- Encourages attention to multiple image regions
- Reduces reliance on specific image areas

8.6 MIXED PRECISION TRAINING

```python
if scaler is not None:
    with torch.amp.autocast('cuda'):
        outputs = model(data)
        # ... loss calculation ...
    
    scaler.scale(loss).backward()
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)
    scaler.step(optimizer)
    scaler.update()
```

Mixed precision benefits:
- Faster training on modern GPUs
- Reduced memory usage
- Maintains numerical stability
- Automatic loss scaling

Process:
1. Forward pass in 16-bit precision
2. Scale loss to prevent underflow
3. Backward pass with scaled gradients
4. Unscale gradients for clipping
5. Apply gradient clipping
6. Update parameters
7. Update loss scaler

8.7 GRADIENT CLIPPING

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)
```

Purpose:
- Prevents exploding gradients
- Ensures stable training
- Particularly important with advanced augmentations
- Max norm of 0.5 for tight control

===============================================================================
9. VALIDATION AND METRICS
===============================================================================

9.1 VALIDATION MODE

```python
model.eval()
```

Effects:
- Disables dropout layers
- Fixes batch normalization statistics
- Sets deterministic behavior
- Consistent evaluation results

9.2 GRADIENT COMPUTATION CONTROL

```python
with torch.no_grad():
    # validation code
```

Benefits:
- Disables gradient computation
- Saves memory during validation
- Faster inference
- Prevents accidental parameter updates

9.3 CONFUSION MATRIX CALCULATION

```python
for t, p in zip(targets.view(-1), predicted.view(-1)):
    if t.long() == 1 and p.long() == 1:
        val_tp += 1  # True Positive
    elif t.long() == 1 and p.long() == 0:
        val_fn += 1  # False Negative
    elif t.long() == 0 and p.long() == 1:
        val_fp += 1  # False Positive
    else:
        val_tn += 1  # True Negative
```

Confusion matrix components:
- True Positive (TP): Correctly detected spoof
- False Negative (FN): Missed spoof attack
- False Positive (FP): False spoof alarm
- True Negative (TN): Correctly identified live face

9.4 METRICS CALCULATION

```python
val_acc = 100.0 * val_correct / val_total
val_precision = val_tp / (val_tp + val_fp) if (val_tp + val_fp) > 0 else 0
val_recall = val_tp / (val_tp + val_fn) if (val_tp + val_fn) > 0 else 0
```

Key metrics:
- Accuracy: Overall correctness
- Precision: Quality of positive predictions
- Recall: Coverage of actual positives

For anti-spoofing:
- High precision: Few false alarms
- High recall: Catches most attacks
- Balance depends on security requirements

===============================================================================
10. MODEL SAVING AND CHECKPOINTING
===============================================================================

10.1 BEST MODEL TRACKING

```python
if val_acc > best_val_acc:
    best_val_acc = val_acc
    epochs_without_improvement = 0
    torch.save(model.state_dict(), best_model_path)
    print(f"Epoch [{epoch+1}/{EPOCHS}] - New best model! Val Acc: {val_acc:.2f}%")
else:
    epochs_without_improvement += 1
```

Strategy:
- Track best validation accuracy
- Save model when validation improves
- Reset early stopping counter
- Only save state_dict for efficiency

10.2 EARLY STOPPING

```python
if epochs_without_improvement >= PATIENCE:
    print(f"Early stopping triggered after {epoch+1} epochs")
    break
```

Benefits:
- Prevents overfitting
- Saves computational resources
- Automatic training termination
- Patience of 15 epochs allows thorough training

===============================================================================
11. PLOTTING AND VISUALIZATION
===============================================================================

11.1 METRICS LOGGING

```python
metrics_logger = MetricsLogger()
metrics_logger.log_epoch(train_loss, train_acc, val_loss, val_acc, val_precision, val_recall)
```

The MetricsLogger class handles:
- Training loss and accuracy tracking
- Validation metrics recording
- Plot generation and saving
- Result folder organization

11.2 PLOT GENERATION

```python
base_name, result_folder = metrics_logger.save_all_plots()
```

Generated visualizations:
- Training/validation loss curves
- Training/validation accuracy curves
- Precision/recall trends
- Combined metrics dashboard
- Confusion matrix plots

11.3 RESULT ORGANIZATION

The system creates organized result folders:
- Timestamped directories
- Comprehensive plot collections
- Training summaries
- Model checkpoints

===============================================================================
12. ADVANCED TECHNIQUES EXPLAINED
===============================================================================

12.1 REPRODUCIBILITY SETUP

```python
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
```

Ensures:
- Reproducible results across runs
- Consistent random number generation
- Deterministic behavior
- Scientific reproducibility

12.2 GPU OPTIMIZATIONS

```python
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
```

Optimizations:
- cuDNN benchmark finds best algorithms
- TF32 enables faster matrix operations
- Ampere GPU acceleration
- Automatic performance tuning

12.3 MEMORY OPTIMIZATION

- Pin memory for faster transfers
- Non-blocking data movement
- Persistent workers
- Mixed precision training
- Gradient accumulation ready

===============================================================================
CONCLUSION
===============================================================================

This implementation represents a state-of-the-art approach to face anti-spoofing 
using CNN classification. The combination of advanced data augmentation, 
sophisticated loss functions, and optimization techniques provides a robust 
foundation for high-accuracy anti-spoofing systems.

Key innovations:
- Dual loss function approach
- Advanced data augmentation pipeline
- Mixed precision training
- Comprehensive evaluation metrics
- Reproducible experimental setup

The modular design allows for easy experimentation and adaptation to different 
datasets and requirements.

===============================================================================
END OF IMPLEMENTATION GUIDE
===============================================================================
