3.3.1.3 Implementation of Advanced Convolutional Neural Network (CNN)

This document details the implementation of an Advanced Convolutional Neural Network (CNN) for face anti-spoofing, drawing insights from the provided `train_cnn.py` and `model_cnn.py` scripts. This enhanced CNN architecture incorporates advanced techniques like Mixup, CutMix, Focal Loss, and Cosine Annealing with Warm Restarts to achieve maximum accuracy and robust generalization.

3.3.1.3.1 Configuration and Hyperparameters

The training process for the Advanced CNN model is configured with specific hyperparameters, fine-tuned for optimal accuracy, stability, and generalization in face anti-spoofing tasks.

*   **BATCH_SIZE**: 64. A batch size of 64 is used. Smaller batches provide better gradient estimates and improved generalization, especially with limited data.
*   **IMAGE_SIZE**: (128, 128). Input images are resized to 128x128 pixels. This higher resolution captures finer facial details crucial for anti-spoofing (e.g., texture, pores).
*   **EPOCHS**: 100. The model is trained for a maximum of 100 epochs, allowing for thorough convergence.
*   **LEARNING_RATE**: 0.0005. A lower initial learning rate of 0.0005 is set for careful fine-tuning and stable convergence to optimal weights.
*   **WEIGHT_DECAY**: 0.005. L2 regularization is applied with a weight decay of 0.005. This reduced weight decay allows for model flexibility while still preventing overfitting.
*   **SAMPLE_LIMIT**: 5000. For faster experimentation and debugging, the training dataset size can be limited to the first 5000 images. This is a configurable parameter and can be set to 0 to use the full dataset.
*   **PATIENCE**: 15. Early stopping patience is increased to 15 epochs, allowing for more thorough training before early stopping is triggered.
*   **LABEL_SMOOTHING**: 0.15. Label smoothing with a factor of 0.15 is used. A higher value prevents overconfident predictions and improves generalization.
*   **GRADIENT_CLIP_NORM**: 0.5. Gradient clipping with a norm of 0.5 is applied. This tighter clipping ensures stable training in deeper networks or with advanced augmentations.
*   **MIXUP_ALPHA**: 0.4. Mixup alpha parameter for blending training samples. Higher values create more aggressive mixing.
*   **CUTMIX_ALPHA**: 1.0. CutMix alpha for random rectangular cutouts. Helps the model focus on multiple facial regions.
*   **MIXUP_PROB**: 0.5. Probability of applying Mixup or CutMix augmentation, balancing augmented vs. original samples.
*   **NUM_WORKERS**: 10. The number of data loading workers is set to 10 for a faster data pipeline, reducing GPU idle time.
*   **PIN_MEMORY**: True. When running on CUDA, `pin_memory` is set to `True` to enable faster data transfer from CPU to GPU.
*   **PERSISTENT_WORKERS**: True. `persistent_workers` is enabled when `NUM_WORKERS > 0` to keep worker processes alive between epochs, reducing overhead.
*   **HORIZONTAL_FLIP_PROB**: 0.5. Probability for random horizontal flip.
*   **ROTATION_DEGREES**: 15. Increased rotation range for head pose variation.
*   **COLOR_JITTER_BRIGHTNESS**: 0.3, **COLOR_JITTER_CONTRAST**: 0.3, **COLOR_JITTER_SATURATION**: 0.3, **COLOR_JITTER_HUE**: 0.15. Enhanced color jitter parameters for lighting and camera variation.
*   **GRAYSCALE_PROB**: 0.1. Probability for occasional grayscale conversion to reduce color dependency.
*   **RANDOM_ERASING_PROB**: 0.2. Increased occlusion simulation.
*   **GAUSSIAN_BLUR_PROB**: 0.1. Probability for occasional Gaussian blur to simulate camera focus variations.
*   **ELASTIC_TRANSFORM_PROB**: 0.1. Probability for elastic deformation for natural variation.
*   **ADAM_BETA1**: 0.9, **ADAM_BETA2**: 0.999, **ADAM_EPS**: 1e-8. Standard momentum parameters and a small epsilon value for the Adam optimizer.
*   **T_0**: 10, **T_MULT**: 2, **ETA_MIN**: 1e-7. Parameters for Cosine Annealing with Warm Restarts scheduler, defining initial restart period, multiplier, and minimum learning rate.
*   **USE_AMP**: True. Automatic Mixed Precision is enabled for faster training and reduced memory usage on modern GPUs.
*   **TTA_ENABLED**: True, **TTA_CROPS**: 5. Test-Time Augmentation is enabled for inference with 5 augmented crops.

3.3.1.3.2 Advanced Data Augmentation

A comprehensive set of advanced data augmentation techniques is applied to the training data to significantly improve the model's generalization capabilities and resilience to diverse input variations.

*   **Resize and RandomResizedCrop**: Images are initially resized to a slightly larger dimension (e.g., 1.1 times `IMAGE_SIZE`) before being randomly cropped to the target `IMAGE_SIZE` (128, 128) with scale and ratio variations. This simulates variations in face positioning, scale, and aspect ratio.
*   **Random Horizontal Flip**: Applied with a probability of 0.5, this augmentation increases data diversity and helps the model learn features invariant to horizontal reflections.
*   **Random Rotation**: Images are randomly rotated within a range of ±15 degrees with bilinear interpolation, simulating slight head pose variations and camera angle differences.
*   **Color Jitter**: Enhanced variations in brightness (±30%), contrast (±30%), saturation (±30%), and hue (±15%) are introduced. This simulates different lighting conditions and camera characteristics, making the model more robust to environmental changes.
*   **GaussianBlur**: A custom Gaussian blur augmentation is applied with a probability of 0.1, simulating camera focus variations and motion blur.
*   **RandomGrayscale**: Occasional grayscale conversion (p=0.1) is used to reduce the model's dependency on color information.
*   **ToTensor**: Converts PIL Images to PyTorch tensors, scaling pixel values from [0, 255] to [0, 1] and reordering dimensions from HWC to CHW.
*   **Normalization**: Images are normalized using ImageNet statistics (Mean: [0.485, 0.456, 0.406], Std: [0.229, 0.224, 0.225]). This standardizes the input distribution, crucial for efficient training and compatibility with pre-trained models.
*   **RandomErasing**: Applied with a probability of 0.2, this technique randomly erases a rectangular region in the image, simulating occlusions and improving robustness.
*   **Mixup**: Creates new training samples by linearly interpolating between pairs of training examples and their labels. This improves generalization and robustness.
*   **CutMix**: Cuts and pastes rectangular regions between training samples, combining local features from different images. This encourages the model to focus on multiple discriminative features.

3.3.1.3.3 Dataset Splitting and Handling

The dataset handling strategy focuses on reproducibility, balanced class representation, and efficient data loading, crucial for robust anti-spoofing training.

*   **Dataset Split**: The dataset is split into an 80% training set and a 20% validation set using `torch.utils.data.random_split`. A fixed random seed of 42 is used with `torch.Generator().manual_seed(42)` to ensure reproducible splits.
*   **Class Imbalance Handling**: To address class imbalance, inverse-frequency weights are calculated for each class based on the training set's class distribution. These weights are then used to create a `torch.utils.data.WeightedRandomSampler`.
*   **Weighted Random Sampling**: The `WeightedRandomSampler` is used with `replacement=True` to ensure that each training batch contains a balanced representation of classes. This prevents the model from being biased towards the majority class.
*   **Data Loaders**: `DataLoader` instances are created for both training and validation sets. The training `DataLoader` uses the `WeightedRandomSampler`. Both loaders are configured with `NUM_WORKERS`, `PIN_MEMORY`, and `PERSISTENT_WORKERS` for optimized data loading.
*   **CUDNN Benchmark**: If a CUDA-enabled GPU is available, `torch.backends.cudnn.benchmark = True` is enabled. This allows cuDNN to auto-tune the best algorithm for convolutional operations, leading to faster training. `torch.backends.cuda.matmul.allow_tf32 = True` and `torch.backends.cudnn.allow_tf32 = True` are also set for further performance optimizations on compatible GPUs.

3.3.1.3.4 CNN Architecture

The `OptimizedCNN` architecture, as defined in `model_cnn.py`, is a custom CNN designed for face anti-spoofing. While the specific layers are not detailed in `train_cnn.py`, the script indicates it's an "Advanced CNN architecture" and a "Custom CNN architecture" from `YeohLiXiang/model_cnn.py`. It is expected to be optimized for capturing discriminative features for spoof detection.

3.3.1.3.5 Loss Functions and Optimization

The training process employs a combination of robust loss functions and an advanced optimization strategy.

*   **Loss Functions**:
    *   `nn.CrossEntropyLoss`: Used as a primary loss function, configured with `label_smoothing=LABEL_SMOOTHING` (0.15) to prevent overconfident predictions and improve generalization.
    *   `FocalLoss`: An implementation of Focal Loss is used to address class imbalance by down-weighting easy examples and focusing training on hard examples. It is initialized with `alpha=1` and `gamma=2`.
    *   **Combined Loss**: During training, a weighted combination of Cross Entropy Loss (70%) and Focal Loss (30%) is used (`0.7 * ce_loss + 0.3 * focal_loss`) to leverage the benefits of both.

*   **Optimizer**: The `optim.AdamW` optimizer is used for training. AdamW provides better regularization than standard Adam due to its decoupled weight decay.
    *   `lr=LEARNING_RATE`: Initial learning rate of 0.0005.
    *   `weight_decay=WEIGHT_DECAY`: L2 regularization with a weight decay of 0.005.
    *   `betas=(ADAM_BETA1, ADAM_BETA2)`: Standard momentum parameters (0.9, 0.999).
    *   `eps=ADAM_EPS`: A small epsilon value (1e-8) for numerical stability.

*   **Learning Rate Scheduler**: A `torch.optim.lr_scheduler.CosineAnnealingWarmRestarts` scheduler is employed. This advanced scheduler periodically "restarts" training with high learning rates, helping the model escape local minima and achieve better final performance.
    *   `T_0=10`: Initial restart period in epochs.
    *   `T_mult=2`: Restart period multiplier for geometric growth.
    *   `eta_min=1e-7`: Minimum learning rate.

*   **Mixed Precision Training**: Mixed precision training is enabled when a CUDA-enabled GPU is available, using `torch.amp.GradScaler`. This utilizes 16-bit floating-point precision for most operations, significantly speeding up training and reducing memory consumption without sacrificing accuracy. Automatic loss scaling and gradient unscaling are handled by the scaler to maintain numerical stability.

3.3.1.3.6 Training Loop Implementation

The training loop is structured to ensure stability, efficiency, and robust learning, incorporating advanced augmentation and optimization techniques.

*   **Phases**: Each epoch comprises distinct training and validation phases.
    *   **Training Phase**: The model is set to `model.train()` mode. Gradients are zeroed (`optimizer.zero_grad()`), the forward pass is performed (with mixed precision if enabled), and loss is calculated using the combined Cross Entropy and Focal Loss. Backpropagation (`loss.backward()`) and optimizer step (`optimizer.step()`) are executed.
    *   **Validation Phase**: The model is set to `model.eval()` mode, disabling gradient computation (`torch.no_grad()`) and dropout. Only Cross Entropy Loss is used for validation to ensure consistent evaluation.

*   **Advanced Augmentation in Loop**: During the training phase, Mixup or CutMix augmentations are randomly applied to batches with a probability of `MIXUP_PROB` (0.5). The loss calculation is adjusted for these mixed samples using `mixup_criterion`.
*   **Mixed Precision**: Within the training loop, if mixed precision is enabled, the forward pass and loss calculation are wrapped in `torch.amp.autocast(device_type='cuda')`. The `GradScaler` handles scaling the loss before backpropagation and unscaling gradients before the optimizer step.
*   **Gradient Clipping**: `torch.nn.utils.clip_grad_norm_` is applied to the model's parameters with `GRADIENT_CLIP_NORM=0.5` after `scaler.unscale_(optimizer)` (if mixed precision is used) or after `loss.backward()` (otherwise). This prevents exploding gradients.
*   **Scheduler Step**: The learning rate scheduler's `scheduler.step()` method is called at the end of each epoch to update the learning rate according to the Cosine Annealing with Warm Restarts schedule.
*   **Metrics and Logging**: Training loss, accuracy, and current learning rate are printed periodically. At the end of each epoch, validation loss, accuracy, precision, recall, F1-score, MSE, and RMSE are calculated and logged using a `MetricsLogger`. This provides a comprehensive overview of the model's performance. Confusion matrices are also generated.

3.3.1.3.7 Model Saving and Checkpointing

A robust model saving and checkpointing strategy is implemented to ensure efficient and reliable training.

*   **Best Model Tracking**: The validation accuracy is continuously monitored. If the current validation accuracy surpasses the `best_val_acc` recorded so far, the model's `state_dict()` is saved to a file (e.g., `cnn_pytorch.pth`). This ensures that the best-performing model on the validation set is always preserved.
*   **Early Stopping**: An early stopping mechanism is integrated with a `PATIENCE` of 15 epochs. If the validation accuracy does not improve for 15 consecutive epochs, training is automatically terminated. This prevents the model from overfitting and saves computational resources.